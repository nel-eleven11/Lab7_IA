{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjLX29s5Ypq10qTYjQfqRK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nel-eleven11/Lab7_IA/blob/main/lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 7\n",
        "\n",
        "- Nelson García Bravatti\n",
        "- Christian Echeverría\n",
        "- Ricardo Chuy"
      ],
      "metadata": {
        "id": "k9zSaJbWVqM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks 1 - Teoría\n",
        "\n",
        "Responda las siguientes preguntas de forma clara y concisa, pueden subir un PDF o bien dentro del mismo Jupyter\n",
        "Notebook."
      ],
      "metadata": {
        "id": "oOuoAzhxVywz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo.\n",
        "\n",
        "R//\n",
        "\n",
        "El Temporal Difference (TD) Learning es una técnica de aprendizaje por refuerzo que combina ideas de los métodos Monte Carlo y la Programación Dinámica, pero sin requerir un modelo del entorno. Su principal característica es que aprende de manera incremental, utilizando la diferencia entre la estimación actual de valor y la estimación actualizada (basada en la siguiente observación) para ajustar sus estimaciones. A esta diferencia se le denomina error de diferencia temporal (temporal difference error).\n",
        "\n",
        "En el aprendizaje supervisado, cada ejemplo de entrenamiento consta de un conjunto de características de entrada y una etiqueta o valor objetivo definitivo. El modelo intenta minimizar el error entre su predicción y la etiqueta real.\n",
        "En TD Learning (y en el aprendizaje por refuerzo en general), no existe una etiqueta “correcta” inmediata para cada estado o acción, sino que se recibe retroalimentación (recompensa) desde el entorno de forma continua o al finalizar episodios. Además, la “etiqueta” para un estado (o acción) se construye en tiempo real con base en las recompensas futuras y en estimaciones ya existentes para estados sucesivos.\n",
        "\n",
        "El error de diferencia temporal refleja la diferencia entre el valor estimado de la situación actual (basado en nuestra función de valor o acción-valor aprendida hasta el momento) y lo que se observa realmente después de dar un paso en el tiempo (recompensa más el valor estimado del siguiente estado).\n",
        "\n",
        "\n",
        "### 2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación.\n",
        "\n",
        "R//\n",
        "\n",
        "\n",
        "\n",
        "### 3. ¿Qué distingue los juegos de suma cero de los juegos de no suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas.\n",
        "\n",
        "R//\n",
        "\n",
        "Juegos de suma cero:\n",
        "\n",
        "Son aquellos en los que la ganancia de un jugador es exactamente la pérdida del otro. El resultado neto de las ganancias y pérdidas entre todos los jugadores es cero. Un ejemplo es un juego de póker (simplificado, con una cantidad fija de dinero): si un jugador gana 10 fichas, esas 10 fichas las pierden otros jugadores.\n",
        "\n",
        "En estos juegos, la estrategia se centra en maximizar la propia ganancia y minimizar la ganancia del adversario, pues lo que uno gana lo pierde el otro.\n",
        "\n",
        "Juegos de no suma cero:\n",
        "\n",
        "Son aquellos en los que el resultado neto no tiene por qué ser cero; es posible que todos ganen o todos pierdan en mayor o menor medida, o que el reparto de beneficios no sea un simple “traspaso” de un jugador a otro.\n",
        "\n",
        "- Ejemplo: La negociación de precios entre una empresa proveedora y una empresa compradora puede verse como un juego no suma cero. Existe la posibilidad de que ambas partes lleguen a un acuerdo que resulte mutuamente beneficioso (por ejemplo, un precio que sea rentable para el proveedor pero que permita al comprador obtener una ganancia en su comercialización). Incluso se pueden establecer contratos a largo plazo o descuentos por volumen que beneficien a ambas partes.\n",
        "\n",
        "- Consideraciones estratégicas:\n",
        "\n",
        "  - Colaboración y coaliciones: Puede haber incentivos para colaborar, porque los resultados pueden mejorar para ambos.\n",
        "  - Creación de valor: Las partes pueden “hacer crecer el pastel” antes de repartirlo.\n",
        "  - Compromisos y señales: En algunos casos, los jugadores pueden enviar señales de buena voluntad o comprometerse a ciertas acciones para lograr acuerdos beneficiosos.\n",
        "\n",
        "### 4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
        "\n",
        "R//\n",
        "\n",
        "El equilibrio de Nash se aplica de manera muy directa a los juegos simultáneos porque en este tipo de juegos todos los jugadores eligen sus estrategias sin conocer las de los demás.\n",
        "Un perfil de estrategias (una estrategia para cada jugador) está en equilibrio de Nash cuando ningún jugador puede mejorar su pago (o utilidad) cambiando unilateralmente su estrategia, asumiendo que las estrategias de los demás jugadores permanecen fijas.\n",
        "\n",
        "La estabilidad radica en que si un jugador cambiara su estrategia de forma unilateral, sabiendo que los demás no cambian, no obtendría un mejor resultado.\n",
        "Esto implica que, desde la perspectiva de cada jugador, la estrategia elegida es óptima dadas las estrategias de los demás.\n",
        "En un juego simultáneo, el equilibrio de Nash se interpreta como la situación en la que, incluso conociendo (hipotéticamente) la decisión de los demás, ninguno quiere desviarse (porque no ganaría más haciéndolo). Esto genera un estado estable: no hay incentivos individuales para cambiar la decisión propia.\n",
        "\n",
        "\n",
        "\n",
        "### 5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
        "\n",
        "R//\n",
        "\n",
        "Aplicación de TD Learning en entornos dinámicos:\n",
        "\n",
        "- Modelado de secuencias de decisiones: El agente aprende una función de valor (o función Q) que indica la calidad de estar en cierto estado (o de tomar cierta acción) con base en recompensas futuras.\n",
        "- Aprendizaje online e incremental: En muchos entornos reales (p.ej., robótica, control de sistemas, optimización de recursos), se necesita ajustar la política de forma continua, sin tener un modelo explícito de las transiciones. El TD Learning es muy adecuado porque permite aprender paso a paso a medida que se realizan acciones y se observan recompensas.\n",
        "- Adaptación a cambios: Si las dinámicas del entorno cambian con el tiempo, TD Learning puede ajustar sus estimaciones sin reiniciar el proceso de aprendizaje, gracias a su naturaleza incremental y de bootstrapping.\n",
        "\n",
        "Equilibrio entre exploración y explotación:\n",
        "\n",
        "El TD Learning por sí mismo no especifica exactamente cómo se eligen las acciones (eso es parte de la política de toma de decisiones); sin embargo, para aprender una buena política, se suelen combinar algoritmos TD (por ejemplo, Q-Learning o SARSA) con esquemas de exploración-explotación, como ε-greedy, donde con probabilidad ε se toman acciones aleatorias (exploración) y con probabilidad 1−ε se toman acciones greedy (explotación).\n",
        "El reto es elegir cuándo y cuánto explorar (buscar acciones potencialmente mejores que aún no se conocen bien) versus explotar (tomar la acción que hasta ahora parece la mejor).\n",
        "\n",
        "Desafíos en la práctica:\n",
        "\n",
        "- Escalabilidad: En problemas con espacios de estados o de acciones muy grandes, se requiere usar funciones de aproximación (como redes neuronales). Esto introduce complejidades como la necesidad de hiperparámetros y diseños de red adecuados.\n",
        "- Elección de la tasa de aprendizaje (α): Una tasa muy alta puede causar inestabilidad y una muy baja hace el aprendizaje lento.\n",
        "- Diseño de la recompensa: Si la función de recompensa no se alinea bien con el objetivo global, el agente puede aprender comportamientos no deseados.\n",
        "- Exploración suficiente: Garantizar que el agente explore suficientemente el espacio de estados y acciones es complicado, sobre todo en entornos con muchas posibilidades o con un alto costo de exploración.\n",
        "- Convergencia y estabilidad: Al usar aproximadores de función no lineales (p. ej., redes neuronales profundas), el aprendizaje TD puede ser inestable o incluso divergir, lo que hace necesario el uso de técnicas adicionales (tales como target networks, experience replay, etc.)."
      ],
      "metadata": {
        "id": "z98FgIzTV7kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliografía:\n",
        "\n",
        "- Temporal Difference Learning | Saturn Cloud. (2023, August 25). https://saturncloud.io/glossary/temporal-difference-learning/\n",
        "-Chen, J. (2024, June 5). Nash Equilibrium: How it works in Game Theory, examples, plus Prisoner’s Dilemma. Investopedia. https://www.investopedia.com/terms/n/nash-equilibrium.asp\n",
        "- David. (2023, March 27). what is Temporal Difference Learning? - David Kim(Changhun Kim) - Medium. Medium. https://medium.com/@chkim345/what-is-temporal-difference-learning-4c4c040613aa"
      ],
      "metadata": {
        "id": "WFFVlA15Yn7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 - Connect Four"
      ],
      "metadata": {
        "id": "3880FouCV3ur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfNstKBsVlLL"
      },
      "outputs": [],
      "source": []
    }
  ]
}