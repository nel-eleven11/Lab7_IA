{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nel-eleven11/Lab7_IA/blob/main/lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9zSaJbWVqM0"
      },
      "source": [
        "# Laboratorio 7\n",
        "\n",
        "- Nelson García Bravatti\n",
        "- Christian Echeverría\n",
        "- Ricardo Chuy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOuoAzhxVywz"
      },
      "source": [
        "## Tasks 1 - Teoría\n",
        "\n",
        "Responda las siguientes preguntas de forma clara y concisa, pueden subir un PDF o bien dentro del mismo Jupyter\n",
        "Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z98FgIzTV7kV"
      },
      "source": [
        "### 1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo.\n",
        "\n",
        "R//\n",
        "\n",
        "El Temporal Difference Learning es una técnica de aprendizaje por refuerzo que combina ideas de los métodos Monte Carlo y la Programación Dinámica, pero sin requerir un modelo del entorno. Su principal característica es que aprende de manera incremental, utilizando la diferencia entre la estimación actual de valor y la estimación actualizada para ajustar sus estimaciones. A esta diferencia se le denomina error de diferencia temporal.\n",
        "\n",
        "En el aprendizaje supervisado, cada ejemplo de entrenamiento consta de un conjunto de características de entrada y una etiqueta o valor objetivo definitivo. El modelo intenta minimizar el error entre su predicción y la etiqueta real.\n",
        "\n",
        "En TD Learning, no existe una etiqueta “correcta” inmediata para cada estado o acción, sino que se recibe retroalimentación (recompensa) desde el entorno de forma continua o al finalizar episodios. Además, la “etiqueta” para un estado (acción) se construye en tiempo real con base en las recompensas futuras y en estimaciones ya existentes para estados sucesivos.\n",
        "\n",
        "El error de diferencia temporal refleja la diferencia entre el valor estimado de la situación actual (basado en la función de valor o acción-valor aprendida hasta el momento) y lo que se observa realmente después de dar un paso en el tiempo (recompensa más el valor estimado del siguiente estado).\n",
        "\n",
        "\n",
        "### 2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación.\n",
        "\n",
        "R//\n",
        "\n",
        "En los juegos simultáneos, todos los jugadores eligen su acción de manera simultánea o, más precisamente, sin conocer previamente qué acción han tomado los demás. Cada jugador deberá basarse en creencias, estimaciones o conjeturas sobre lo que el oponente (u oponentes) va a hacer y, a la vez, considerar sus propios objetivos para maximizar o satisfacer mejor su utilidad, o minimizar pérdidas.\n",
        "\n",
        "Cómo tomar decisiones:\n",
        "\n",
        "- Formular conjeturas sobre la posible estrategia del rival/rivales.\n",
        "\n",
        "- Escoger la estrategia que mejor responde a las conjeturas hechas.\n",
        "\n",
        "- Dado que la elección es simultánea, ningún jugador puede esperar “ver” la decisión del otro y reaccionar en consecuencia.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "Un caso típico es la subasta sellada de primer precio: Varios licitadores (jugadores) deben presentar una oferta por un objeto por ejemplo, una obra de arte o un contrato público. Cada oferta se entrega sin que ningún licitador sepa la de los demás. Gana el que presente la oferta más alta y paga el precio que ofreció.\n",
        "\n",
        "En este escenario, cada participante estima cuánto podrían ofertar los demás y decide su oferta para tratar de maximizar su beneficio (obtener el objeto a un precio razonable) o su probabilidad de ganar, dependiendo de su estrategia.\n",
        "\n",
        "Estrategias posibles:\n",
        "\n",
        "- Estrategias puras: Hacer siempre una misma acción, por ejemplo, ofertar un monto específico basado en la valoración personal.\n",
        "\n",
        "- Estrategias mixtas: Mezclar probabilísticamente diferentes ofertas, para no ser predecible.\n",
        "\n",
        "- Estrategias de equilibrio (como equilibrio de Nash): Cada jugador elige su oferta óptima dadas las conjeturas (o la distribución de ofertas) del resto.\n",
        "\n",
        "### 3. ¿Qué distingue los juegos de suma cero de los juegos de no suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas.\n",
        "\n",
        "R//\n",
        "\n",
        "Juegos de suma cero:\n",
        "\n",
        "Son aquellos en los que la ganancia de un jugador es exactamente la pérdida del otro. El resultado neto de las ganancias y pérdidas entre todos los jugadores es cero. Un ejemplo es un juego de póker: si un jugador gana 10 fichas, esas 10 fichas las pierden otros jugadores.\n",
        "\n",
        "En estos juegos, la estrategia se centra en maximizar la propia ganancia y minimizar la ganancia del adversario, pues lo que uno gana lo pierde el otro.\n",
        "\n",
        "Juegos de no suma cero:\n",
        "\n",
        "Son aquellos en los que el resultado neto no tiene por qué ser cero; es posible que todos ganen o todos pierdan en mayor o menor medida, o que el reparto de beneficios no sea un simple “traspaso” de un jugador a otro.\n",
        "\n",
        "- Ejemplo: La negociación de precios entre una empresa proveedora y una empresa compradora puede verse como un juego no suma cero. Existe la posibilidad de que ambas partes lleguen a un acuerdo que resulte mutuamente beneficioso, por ejemplo, un precio que sea rentable para el proveedor pero que permita al comprador obtener una ganancia en su comercialización. Incluso se pueden establecer contratos a largo plazo o descuentos por volumen que beneficien a ambas partes.\n",
        "\n",
        "- Consideraciones estratégicas:\n",
        "\n",
        "  - Colaboración y coaliciones: Puede haber incentivos para colaborar, porque los resultados pueden mejorar para ambos.\n",
        "  - Creación de valor: Las partes pueden “hacer crecer el pastel” antes de repartirlo.\n",
        "  - Compromisos y señales: En algunos casos, los jugadores pueden enviar señales de buena voluntad o comprometerse a ciertas acciones para lograr acuerdos beneficiosos.\n",
        "\n",
        "### 4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
        "\n",
        "R//\n",
        "\n",
        "El equilibrio de Nash se aplica de manera muy directa a los juegos simultáneos porque en este tipo de juegos todos los jugadores eligen sus estrategias sin conocer las de los demás.\n",
        "\n",
        "Un perfil de estrategias (una estrategia para cada jugador) está en equilibrio de Nash cuando ningún jugador puede mejorar su pago (o utilidad) cambiando unilateralmente su estrategia, asumiendo que las estrategias de los demás jugadores permanecen fijas.\n",
        "\n",
        "La estabilidad radica en que si un jugador cambiara su estrategia de forma unilateral, sabiendo que los demás no cambian, no obtendría un mejor resultado.\n",
        "Esto implica que, desde la perspectiva de cada jugador, la estrategia elegida es óptima dadas las estrategias de los demás.\n",
        "En un juego simultáneo, el equilibrio de Nash se interpreta como la situación en la que, incluso conociendo (hipotéticamente) la decisión de los demás, ninguno quiere desviarse (porque no ganaría más haciéndolo). Esto genera un estado estable: no hay incentivos individuales para cambiar la decisión propia.\n",
        "\n",
        "\n",
        "### 5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
        "\n",
        "R//\n",
        "\n",
        "Aplicación de TD Learning en entornos dinámicos:\n",
        "\n",
        "- Modelado de secuencias de decisiones: El agente aprende una función de valor (función Q) que indica la calidad de estar en cierto estado (o de tomar cierta acción) con base en recompensas futuras.\n",
        "- Aprendizaje online e incremental: En muchos entornos reales, se necesita ajustar la política de forma continua, sin tener un modelo explícito de las transiciones. El TD Learning es muy adecuado porque permite aprender paso a paso a medida que se realizan acciones y se observan recompensas.\n",
        "- Adaptación a cambios: Si las dinámicas del entorno cambian con el tiempo, TD Learning puede ajustar sus estimaciones sin reiniciar el proceso de aprendizaje, gracias a su naturaleza incremental y de bootstrapping.\n",
        "\n",
        "Equilibrio entre exploración y explotación:\n",
        "\n",
        "El TD Learning por sí mismo no especifica exactamente cómo se eligen las acciones; sin embargo, para aprender una buena política, se suelen combinar algoritmos TD (por ejemplo, Q-Learning o SARSA) con esquemas de exploración-explotación, como ε-greedy, donde con probabilidad ε se toman acciones aleatorias y con probabilidad 1−ε se toman acciones greedy.\n",
        "\n",
        "El reto es elegir cuándo y cuánto explorar, buscar acciones potencialmente mejores que aún no se conocen bien; versus explotar, tomar la acción que hasta ahora parece la mejor.\n",
        "\n",
        "Desafíos en la práctica:\n",
        "\n",
        "- Escalabilidad: En problemas con espacios de estados o de acciones muy grandes, se requiere usar funciones de aproximación (como redes neuronales). Esto introduce complejidades como la necesidad de hiperparámetros y diseños de red adecuados.\n",
        "- Elección de la tasa de aprendizaje (α): Una tasa muy alta puede causar inestabilidad y una muy baja hace el aprendizaje lento.\n",
        "- Diseño de la recompensa: Si la función de recompensa no se alinea bien con el objetivo global, el agente puede aprender comportamientos no deseados.\n",
        "- Exploración suficiente: Garantizar que el agente explore suficientemente el espacio de estados y acciones es complicado, sobre todo en entornos con muchas posibilidades o con un alto costo de exploración.\n",
        "- Convergencia y estabilidad: Al usar aproximadores de función no lineales (p. ej., redes neuronales profundas), el aprendizaje TD puede ser inestable o incluso divergir, lo que hace necesario el uso de técnicas adicionales (tales como target networks, experience replay, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFFVlA15Yn7o"
      },
      "source": [
        "### Bibliografía:\n",
        "\n",
        "- Temporal Difference Learning | Saturn Cloud. (2023, August 25). https://saturncloud.io/glossary/temporal-difference-learning/\n",
        "-Chen, J. (2024, June 5). Nash Equilibrium: How it works in Game Theory, examples, plus Prisoner’s Dilemma. Investopedia. https://www.investopedia.com/terms/n/nash-equilibrium.asp\n",
        "- David. (2023, March 27). what is Temporal Difference Learning? - David Kim(Changhun Kim) - Medium. Medium. https://medium.com/@chkim345/what-is-temporal-difference-learning-4c4c040613aa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3880FouCV3ur"
      },
      "source": [
        "## Task 2 - Connect Four"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rfNstKBsVlLL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_win_board(board, piece):\n",
        "    rows, cols = board.shape\n",
        "    # Comprobar horizontal\n",
        "    for r in range(rows):\n",
        "        for c in range(cols - 3):\n",
        "            if board[r][c] == piece and board[r][c+1] == piece and \\\n",
        "               board[r][c+2] == piece and board[r][c+3] == piece:\n",
        "                return True\n",
        "    # Comprobar vertical\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols):\n",
        "            if board[r][c] == piece and board[r+1][c] == piece and \\\n",
        "               board[r+2][c] == piece and board[r+3][c] == piece:\n",
        "                return True\n",
        "    # Diagonal con pendiente negativa (hacia arriba)\n",
        "    for r in range(3, rows):\n",
        "        for c in range(cols - 3):\n",
        "            if board[r][c] == piece and board[r-1][c+1] == piece and \\\n",
        "               board[r-2][c+2] == piece and board[r-3][c+3] == piece:\n",
        "                return True\n",
        "    # Diagonal con pendiente positiva (hacia abajo)\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            if board[r][c] == piece and board[r+1][c+1] == piece and \\\n",
        "               board[r+2][c+2] == piece and board[r+3][c+3] == piece:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def get_valid_locations(board):\n",
        "    valid_locations = []\n",
        "    cols = board.shape[1]\n",
        "    for col in range(cols):\n",
        "        if board[0][col] == 0:\n",
        "            valid_locations.append(col)\n",
        "    return valid_locations\n",
        "\n",
        "def get_next_open_row(board, col):\n",
        "    rows = board.shape[0]\n",
        "    for r in range(rows - 1, -1, -1):\n",
        "        if board[r][col] == 0:\n",
        "            return r\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entorno de conecta 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Connect4Env:\n",
        "    def __init__(self):\n",
        "        self.rows = 6\n",
        "        self.cols = 7\n",
        "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
        "        self.current_player = 1  # Jugador 1 comienza\n",
        "        self.game_over = False\n",
        "        self.winner = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.game_over = False\n",
        "        self.winner = None\n",
        "        return self.board\n",
        "\n",
        "    def is_valid_move(self, col):\n",
        "        return self.board[0][col] == 0\n",
        "\n",
        "    def make_move(self, col):\n",
        "        if not self.is_valid_move(col):\n",
        "            return False\n",
        "\n",
        "        # Se deposita la ficha en la fila más baja disponible\n",
        "        for row in range(self.rows - 1, -1, -1):\n",
        "            if self.board[row][col] == 0:\n",
        "                self.board[row][col] = self.current_player\n",
        "                break\n",
        "\n",
        "        # Se guarda el jugador que realizó el movimiento\n",
        "        player_moved = self.current_player\n",
        "\n",
        "        # Verifica si el movimiento produjo victoria o empate\n",
        "        if check_win_board(self.board, player_moved):\n",
        "            self.game_over = True\n",
        "            self.winner = player_moved\n",
        "        elif self.is_board_full():\n",
        "            self.game_over = True\n",
        "            self.winner = 0  # Empate\n",
        "\n",
        "        # Cambiar turno (1 <-> 2)\n",
        "        self.current_player = 3 - self.current_player\n",
        "        return True\n",
        "\n",
        "    def is_board_full(self):\n",
        "        return np.all(self.board != 0)\n",
        "\n",
        "    def render(self):\n",
        "        # Imprime el tablero de forma que la base quede abajo\n",
        "        #print(np.flip(self.board, 0))\n",
        "        print(self.board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones de heuristica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_window(window, piece):\n",
        "    score = 0\n",
        "    opp_piece = 3 - piece\n",
        "    if window.count(piece) == 4:\n",
        "        score += 100\n",
        "    elif window.count(piece) == 3 and window.count(0) == 1:\n",
        "        score += 5\n",
        "    elif window.count(piece) == 2 and window.count(0) == 2:\n",
        "        score += 2\n",
        "\n",
        "    if window.count(opp_piece) == 3 and window.count(0) == 1:\n",
        "        score -= 4\n",
        "\n",
        "    return score\n",
        "\n",
        "def score_position(board, piece):\n",
        "    score = 0\n",
        "    rows, cols = board.shape\n",
        "\n",
        "    # Prioridad al centro\n",
        "    center_array = [int(i) for i in list(board[:, cols // 2])]\n",
        "    center_count = center_array.count(piece)\n",
        "    score += center_count * 3\n",
        "\n",
        "    # Evaluar horizontales\n",
        "    for r in range(rows):\n",
        "        row_array = [int(i) for i in list(board[r, :])]\n",
        "        for c in range(cols - 3):\n",
        "            window = row_array[c:c + 4]\n",
        "            score += score_window(window, piece)\n",
        "\n",
        "    # Evaluar verticales\n",
        "    for c in range(cols):\n",
        "        col_array = [int(i) for i in list(board[:, c])]\n",
        "        for r in range(rows - 3):\n",
        "            window = col_array[r:r + 4]\n",
        "            score += score_window(window, piece)\n",
        "\n",
        "    # Evaluar diagonales (pendiente positiva)\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            window = [board[r + i][c + i] for i in range(4)]\n",
        "            score += score_window(window, piece)\n",
        "\n",
        "    # Evaluar diagonales (pendiente negativa)\n",
        "    for r in range(3, rows):\n",
        "        for c in range(cols - 3):\n",
        "            window = [board[r - i][c + i] for i in range(4)]\n",
        "            score += score_window(window, piece)\n",
        "\n",
        "    return score\n",
        "\n",
        "def is_terminal_node(board):\n",
        "    return check_win_board(board, 1) or check_win_board(board, 2) or len(get_valid_locations(board)) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def minimax(board, depth, alpha, beta, maximizingPlayer, use_alpha_beta, piece):\n",
        "    valid_locations = get_valid_locations(board)\n",
        "    terminal = is_terminal_node(board)\n",
        "    if depth == 0 or terminal:\n",
        "        if terminal:\n",
        "            if check_win_board(board, piece):\n",
        "                return (None, 100000000000)\n",
        "            elif check_win_board(board, 3 - piece):\n",
        "                return (None, -100000000000)\n",
        "            else:\n",
        "                return (None, 0)\n",
        "        else:\n",
        "            return (None, score_position(board, piece))\n",
        "    if maximizingPlayer:\n",
        "        value = -math.inf\n",
        "        best_col = random.choice(valid_locations)\n",
        "        for col in valid_locations:\n",
        "            row = get_next_open_row(board, col)\n",
        "            b_copy = board.copy()\n",
        "            b_copy[row][col] = piece\n",
        "            new_score = minimax(b_copy, depth - 1, alpha, beta, False, use_alpha_beta, piece)[1]\n",
        "            if new_score > value:\n",
        "                value = new_score\n",
        "                best_col = col\n",
        "            if use_alpha_beta:\n",
        "                alpha = max(alpha, value)\n",
        "                if alpha >= beta:\n",
        "                    break\n",
        "        return best_col, value\n",
        "    else:\n",
        "        value = math.inf\n",
        "        best_col = random.choice(valid_locations)\n",
        "        for col in valid_locations:\n",
        "            row = get_next_open_row(board, col)\n",
        "            b_copy = board.copy()\n",
        "            b_copy[row][col] = 3 - piece\n",
        "            new_score = minimax(b_copy, depth - 1, alpha, beta, True, use_alpha_beta, piece)[1]\n",
        "            if new_score < value:\n",
        "                value = new_score\n",
        "                best_col = col\n",
        "            if use_alpha_beta:\n",
        "                beta = min(beta, value)\n",
        "                if alpha >= beta:\n",
        "                    break\n",
        "        return best_col, value\n",
        "\n",
        "def get_best_move(board, depth, use_alpha_beta, piece):\n",
        "    col, score = minimax(board, depth, -math.inf, math.inf, True, use_alpha_beta, piece)\n",
        "    return col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementación de TD(temporal difference learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para esta parte elegimos el q-learning para hacer el agente y aplicaremos elementos e-greedy para poder obtener la tabla optima para el agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero convertiremos el tablero en algo más entendible para el algoritmo de Q - greedy, lo cual en este caso será un diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_state_key(board):\n",
        "    return tuple(board.flatten())\n",
        "\n",
        "Q_table = {}\n",
        "\n",
        "def initialize_q_state(state_key, valid_actions, Q_table):\n",
        "    if state_key not in Q_table:\n",
        "        Q_table[state_key] = {action: 0.0 for action in valid_actions}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Función para el uso de e - greedy \n",
        "def choose_action(state_key, valid_actions, Q_table, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(valid_actions)\n",
        "    \n",
        "    initialize_q_state(state_key, valid_actions, Q_table)\n",
        "    q_values = Q_table[state_key]\n",
        "    \n",
        "    max_q = max(q_values[a] for a in valid_actions)\n",
        "    best_actions = [a for a in valid_actions if q_values[a] == max_q]\n",
        "    \n",
        "    return random.choice(best_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos con el update de la tabla para que encuentre el mejor movimiento posible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_q_value(Q_table, state_key, action, reward, next_state_key, next_valid_actions, alpha, gamma):\n",
        "    initialize_q_state(next_state_key, next_valid_actions, Q_table)\n",
        "    \n",
        "    future_q = max(Q_table[next_state_key][a] for a in next_valid_actions) if next_valid_actions else 0\n",
        "    current_q = Q_table[state_key][action]\n",
        "    \n",
        "    Q_table[state_key][action] = current_q + alpha * (reward + gamma * future_q - current_q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamiento para realizar la Q - table optima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_opponent_move(board):\n",
        "    return random.choice(get_valid_locations(board))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_q_agent_modular(Q_table, episodes=10000, alpha=0.1, gamma=0.95,\n",
        "                          epsilon=1.0, epsilon_decay=0.9995, min_epsilon=0.1,\n",
        "                          log_interval=500, print_logs=True,\n",
        "                          opponent_type='random', depth=1):\n",
        "\n",
        "    stats = {\n",
        "        \"episodes\": [],\n",
        "        \"wins\": [],\n",
        "        \"losses\": [],\n",
        "        \"draws\": []\n",
        "    }\n",
        "\n",
        "    wins = losses = draws = 0\n",
        "\n",
        "    for episode in range(1, episodes + 1):\n",
        "        game = Connect4Env()\n",
        "        game.reset()\n",
        "        done = False\n",
        "        current_player = random.choice([1, 2])  # Quien inicia\n",
        "\n",
        "        while not done:\n",
        "            if current_player == 1:  # TD Agent\n",
        "                state_key = get_state_key(game.board)\n",
        "                valid_actions = get_valid_locations(game.board)\n",
        "                initialize_q_state(state_key, valid_actions, Q_table)\n",
        "                action = choose_action(state_key, valid_actions, Q_table, epsilon)\n",
        "            else:\n",
        "                if opponent_type == 'random':\n",
        "                    action = random_opponent_move(game.board)\n",
        "                elif opponent_type == 'minimax':\n",
        "                    action = get_best_move(game.board.copy(), depth, use_alpha_beta=False, piece=2)\n",
        "\n",
        "            game.make_move(action)\n",
        "            done = game.game_over\n",
        "\n",
        "            if current_player == 1:\n",
        "                next_state_key = get_state_key(game.board)\n",
        "                next_valid_actions = get_valid_locations(game.board)\n",
        "                reward = 0\n",
        "\n",
        "                if game.game_over:\n",
        "                    if game.winner == 1:\n",
        "                        reward = 1\n",
        "                    elif game.winner == 2:\n",
        "                        reward = -1\n",
        "                    else:\n",
        "                        reward = 0\n",
        "\n",
        "                update_q_value(Q_table, state_key, action, reward, next_state_key, next_valid_actions, alpha, gamma)\n",
        "                state_key = next_state_key\n",
        "\n",
        "            current_player = 3 - current_player\n",
        "\n",
        "        # Estadísticas por bloque\n",
        "        if game.winner == 1:\n",
        "            wins += 1\n",
        "        elif game.winner == 2:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "        if epsilon > min_epsilon:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % log_interval == 0:\n",
        "            stats[\"episodes\"].append(episode)\n",
        "            stats[\"wins\"].append(wins)\n",
        "            stats[\"losses\"].append(losses)\n",
        "            stats[\"draws\"].append(draws)\n",
        "            if print_logs:\n",
        "                print(f\"[{episode}] Wins: {wins}, Losses: {losses}, Draws: {draws}, Epsilon: {epsilon:.4f}\")\n",
        "            wins = losses = draws = 0\n",
        "\n",
        "    # Gráfica de entrenamiento\n",
        "    plt.plot(stats[\"episodes\"], stats[\"wins\"], label=\"Victorias\", marker='o')\n",
        "    plt.plot(stats[\"episodes\"], stats[\"losses\"], label=\"Derrotas\", marker='x')\n",
        "    plt.plot(stats[\"episodes\"], stats[\"draws\"], label=\"Empates\", marker='s')\n",
        "    plt.xlabel(\"Episodios\")\n",
        "    plt.ylabel(\"Cantidad\")\n",
        "    plt.title(\"Desempeño del agente TD durante el entrenamiento\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def evaluate_agent(Q_table, episodes=50, depth=3, use_alpha_beta=False, opponent_type='minimax'):\n",
        "    td_wins = 0\n",
        "    opp_wins = 0\n",
        "    draws = 0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        game = Connect4Env()\n",
        "        game.reset()\n",
        "        done = False\n",
        "        current_player = 1  # TD empieza\n",
        "\n",
        "        while not done:\n",
        "            if current_player == 1:\n",
        "                state_key = get_state_key(game.board)\n",
        "                valid_actions = get_valid_locations(game.board)\n",
        "                initialize_q_state(state_key, valid_actions, Q_table)\n",
        "                action = choose_action(state_key, valid_actions, Q_table, epsilon=0.0)\n",
        "            else:\n",
        "                if opponent_type == 'minimax':\n",
        "                    action = get_best_move(game.board.copy(), depth, use_alpha_beta, piece=2)\n",
        "\n",
        "            game.make_move(action)\n",
        "            current_player = 3 - current_player\n",
        "            done = game.game_over\n",
        "\n",
        "        if game.winner == 1:\n",
        "            td_wins += 1\n",
        "        elif game.winner == 2:\n",
        "            opp_wins += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    return td_wins, opp_wins, draws\n",
        "\n",
        "results = {\n",
        "    \"Minimax\": evaluate_agent(Q_table, episodes=50, use_alpha_beta=False),\n",
        "    \"Minimax AB\": evaluate_agent(Q_table, episodes=50, use_alpha_beta=True)\n",
        "}\n",
        "\n",
        "labels = ['TD Agent', 'Minimax', 'Empates']\n",
        "x = list(results.keys())\n",
        "td_values = [results[k][0] for k in x]\n",
        "opp_values = [results[k][1] for k in x]\n",
        "draws = [results[k][2] for k in x]\n",
        "\n",
        "bar_width = 0.25\n",
        "r1 = range(len(x))\n",
        "r2 = [i + bar_width for i in r1]\n",
        "r3 = [i + bar_width*2 for i in r1]\n",
        "\n",
        "plt.bar(r1, td_values, width=bar_width, label='TD Agent')\n",
        "plt.bar(r2, opp_values, width=bar_width, label='Oponente')\n",
        "plt.bar(r3, draws, width=bar_width, label='Empates')\n",
        "\n",
        "plt.xticks([r + bar_width for r in range(len(x))], x)\n",
        "plt.ylabel(\"Número de partidas\")\n",
        "plt.title(\"Resultados del TD Agent vs Oponentes\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Entrenar el agente TD contra Minimax depth=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "Q_table = {}\n",
        "train_stats = train_q_agent_modular(\n",
        "    Q_table,\n",
        "    episodes=10000,\n",
        "    opponent_type='minimax',\n",
        "    depth=1,             # Dificultad baja del oponente\n",
        "    alpha=0.1,\n",
        "    gamma=0.95,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.9995,\n",
        "    min_epsilon=0.1,\n",
        "    print_logs=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Guardar la Q-table en una carpeta segura\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/q_table_minimax_depth1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(Q_table, f)\n",
        "print(\"✅ Q-table guardada como 'data/q_table_minimax_depth1.pkl'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Evaluar desempeño contra Minimax y Minimax con poda\n",
        "results = {\n",
        "    \"Minimax\": evaluate_agent(Q_table, episodes=50, use_alpha_beta=False),\n",
        "    \"Minimax AB\": evaluate_agent(Q_table, episodes=50, use_alpha_beta=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grafico 1\n",
        "labels = ['TD Agent', 'Minimax', 'Empates']\n",
        "x = list(results.keys())\n",
        "td_values = [results[k][0] for k in x]\n",
        "opp_values = [results[k][1] for k in x]\n",
        "draws = [results[k][2] for k in x]\n",
        "\n",
        "bar_width = 0.25\n",
        "r1 = range(len(x))\n",
        "r2 = [i + bar_width for i in r1]\n",
        "r3 = [i + bar_width*2 for i in r1]\n",
        "\n",
        "plt.bar(r1, td_values, width=bar_width, label='TD Agent')\n",
        "plt.bar(r2, opp_values, width=bar_width, label='Oponente')\n",
        "plt.bar(r3, draws, width=bar_width, label='Empates')\n",
        "\n",
        "plt.xticks([r + bar_width for r in range(len(x))], x)\n",
        "plt.ylabel(\"Número de partidas\")\n",
        "plt.title(\"Resultados del TD Agent vs Oponentes\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Segundo grafico\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_stats[\"episodes\"], train_stats[\"wins\"], label=\"Victorias TD\", marker='o')\n",
        "plt.plot(train_stats[\"episodes\"], train_stats[\"losses\"], label=\"Derrotas TD\", marker='x')\n",
        "plt.plot(train_stats[\"episodes\"], train_stats[\"draws\"], label=\"Empates\", marker='s')\n",
        "plt.xlabel(\"Episodios\")\n",
        "plt.ylabel(\"Cantidad por bloque\")\n",
        "plt.title(\"📈 Evolución del rendimiento del TD Agent\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grafico 3\n",
        "import numpy as np\n",
        "\n",
        "labels = list(results.keys())\n",
        "td = np.array([results[k][0] for k in labels])\n",
        "opp = np.array([results[k][1] for k in labels])\n",
        "draw = np.array([results[k][2] for k in labels])\n",
        "\n",
        "total = td + opp + draw\n",
        "td_pct = td / total * 100\n",
        "opp_pct = opp / total * 100\n",
        "draw_pct = draw / total * 100\n",
        "\n",
        "bar_width = 0.35\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(x, td_pct, bar_width, label=\"TD Agent\")\n",
        "plt.bar(x, opp_pct, bar_width, bottom=td_pct, label=\"Oponente\")\n",
        "plt.bar(x, draw_pct, bar_width, bottom=td_pct+opp_pct, label=\"Empates\")\n",
        "\n",
        "plt.ylabel(\"% de partidas\")\n",
        "plt.title(\"📊 Porcentaje de resultados del TD Agent vs Oponentes\")\n",
        "plt.xticks(x, labels)\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMeT//+vPxQNv4MhEx22Xto",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
